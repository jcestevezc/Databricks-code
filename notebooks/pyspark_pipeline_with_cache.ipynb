{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Data Pipeline Example with Caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates a simple data pipeline in PySpark and shows common use cases for the `.cache()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lower, trim, regexp_replace, current_timestamp\nfrom pyspark.sql.types import StringType, IntegerType, StructType, StructField\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"Data Quality Pipeline\")     .getOrCreate()\n\n# Configure logging\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"email\", StringType(), True),\n    StructField(\"signup_date\", StringType(), True)\n])\n\ndf = spark.read.csv(\"path/to/file.csv\", schema=schema, header=True)\nlogger.info(\"Data loaded successfully\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning and Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = df.withColumn(\"name\", trim(lower(col(\"name\"))))\ndf = df.withColumn(\"email\", regexp_replace(col(\"email\"), \"\\s+\", \"\"))\ndf = df.withColumn(\"signup_date\", regexp_replace(col(\"signup_date\"), \"/\", \"-\"))\ndf = df.withColumn(\"ingestion_time\", current_timestamp())\nlogger.info(\"Data cleaned and transformed\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def validate_email(email):\n    import re\n    if re.match(r\"[^@]+@[^@]+\\.[^@]+\", email):\n        return email\n    return None\n\nfrom pyspark.sql.functions import udf\nvalidate_email_udf = udf(validate_email, StringType())\n\ndf = df.withColumn(\"valid_email\", validate_email_udf(col(\"email\")))\ndf = df.filter(col(\"valid_email\").isNotNull())\nlogger.info(\"Data validated\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Enrichment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = df.withColumn(\"name_length\", col(\"name\").cast(StringType()).rlike(\"^[a-zA-Z]+$\"))\nlogger.info(\"Data enriched\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Aggregation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "result = df.groupBy(\"name_length\").avg(\"age\")\nlogger.info(\"Aggregation and analysis complete\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Saving Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "result.write.mode(\"overwrite\").parquet(\"path/to/output/folder\")\nlogger.info(\"Data saved successfully\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Monitoring and Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "logger.info(\"Pipeline completed successfully\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using `.cache()` in PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `.cache()` method stores a DataFrame or RDD in memory so that subsequent actions reuse the cached data instead of recomputing it. This is helpful when the same dataset is accessed multiple times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Repeated Operations on the Same DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = spark.read.csv(\"path/to/large_file.csv\", header=True, inferSchema=True)\n\ndf.cache()\n\ndf.count()\ndf.groupBy(\"column_name\").count().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Exploration and Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = spark.read.parquet(\"path/to/data.parquet\")\n\ndf.cache()\n\ndf.describe().show()\ndf.groupBy(\"column_name\").agg({\"column_name\": \"mean\"}).show()\ndf.filter(df[\"column_name\"] > 100).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. ETL Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = spark.read.json(\"path/to/data.json\")\n\ndf = df.withColumn(\"new_column\", df[\"existing_column\"] * 2)\n\ndf.cache()\n\ndf = df.filter(df[\"new_column\"] > 100)\n\ndf.write.parquet(\"path/to/output.parquet\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Iterative Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\ndf = spark.read.parquet(\"path/to/data.parquet\")\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\ndf = assembler.transform(df).select(\"features\")\n\ndf.cache()\n\nkmeans = KMeans(k=3, seed=1)\nmodel = kmeans.fit(df)\n\npredictions = model.transform(df)\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(f\"Silhouette score: {silhouette}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Join Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df1 = spark.read.csv(\"path/to/data1.csv\", header=True, inferSchema=True)\ndf2 = spark.read.csv(\"path/to/data2.csv\", header=True, inferSchema=True)\ndf3 = spark.read.csv(\"path/to/data3.csv\", header=True, inferSchema=True)\n\ndf1.cache()\n\njoined_df1 = df1.join(df2, \"key_column\")\njoined_df2 = df1.join(df3, \"key_column\")\n\njoined_df1.show()\njoined_df2.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Data Subsets for Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = spark.read.csv(\"path/to/data.csv\", header=True, inferSchema=True)\n\ntrain_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n\ntrain_df.cache()\ntest_df.cache()\n\nmodel = train_model(train_df)\nevaluate_model(model, test_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Releasing Cached Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.unpersist()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
