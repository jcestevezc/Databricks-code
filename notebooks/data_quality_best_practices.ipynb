{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Best Practices for Data Quality Pipelines in PySpark"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 1. Standardization and Normalization of Data\n",
              "- Consistent formatting: ensure uniform formats for dates, column names, and data types.\n",
              "- Normalization: convert values to a common format such as lowercasing and trimming whitespace."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from pyspark.sql.functions import trim, lower, col\n",
              "\n",
              "df = df.withColumn(\"column_name\", trim(lower(col(\"column_name\"))))"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 2. Data Validation\n",
              "- Strict schemas: define explicit schemas for DataFrames to enforce data types.\n",
              "- Range and format checks: validate that values fall within expected ranges and match required patterns."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
              "\n",
              "schema = StructType([\n",
              "    StructField(\"name\", StringType(), True),\n",
              "    StructField(\"age\", IntegerType(), True)\n",
              "])\n",
              "\n",
              "df = spark.read.schema(schema).csv(\"path/to/file.csv\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3. Data Cleaning\n",
              "- Removing duplicates to ensure uniqueness.\n",
              "- Handling nulls: drop them, fill with defaults, or use imputation."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["df = df.dropDuplicates()\n",
              "df = df.fillna({\"column_name\": \"default_value\"})"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 4. Logging and Monitoring\n",
              "- Record errors and warnings during processing.\n",
              "- Continuous monitoring of pipeline status and performance."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["import logging\n",
              "\n",
              "logging.basicConfig(level=logging.INFO)\n",
              "logger = logging.getLogger(__name__)\n",
              "\n",
              "try:\n",
              "    # data processing code\n",
              "    logger.info(\"Processing succeeded\")\n",
              "except Exception as e:\n",
              "    logger.error(f\"Processing error: {e}\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 5. Data Versioning\n",
              "- Version control for datasets to ensure traceability.\n",
              "- Timestamp records to track creation or update times."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from pyspark.sql.functions import current_timestamp\n",
              "\n",
              "df = df.withColumn(\"ingestion_time\", current_timestamp())"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 6. Scalability and Performance\n",
              "- Partition data to speed up queries.\n",
              "- Optimize transformations and avoid expensive operations."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["df = df.repartition(10, \"partition_column\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 7. Documentation and Comments\n",
              "- Document code clearly to explain each step.\n",
              "- Use inline comments for complex logic."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["# Repartition DataFrame to improve query performance\n",
              "df = df.repartition(10, \"partition_column\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 8. Testing and Continuous Validation\n",
              "- Unit tests for transformation logic.\n",
              "- Integration tests to ensure pipeline components work together."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["import unittest\n",
              "\n",
              "class TestDataPipeline(unittest.TestCase):\n",
              "    def test_transform(self):\n",
              "        input_df = spark.createDataFrame([(\"example\",)], [\"column_name\"])\n",
              "        expected_df = spark.createDataFrame([(\"EXAMPLE\",)], [\"column_name\"])\n",
              "        result_df = transform(input_df)  # transform function to test\n",
              "        self.assertEqual(expected_df.collect(), result_df.collect())\n",
              "\n",
              "if __name__ == \"__main__\":\n",
              "    unittest.main()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 9. Maintenance and Evolution\n",
              "- Refactor regularly for readability and maintainability.\n",
              "- Adapt pipelines to new business requirements or data characteristics."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 10. Security and Privacy\n",
              "- Encrypt sensitive data to protect privacy.\n",
              "- Implement access controls to prevent unauthorized use."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from cryptography.fernet import Fernet\n",
              "\n",
              "key = Fernet.generate_key()\n",
              "cipher_suite = Fernet(key)\n",
              "\n",
              "encrypted_text = cipher_suite.encrypt(b'sensitive_data')\n",
              "decrypted_text = cipher_suite.decrypt(encrypted_text)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 11. Task Automation\n",
              "- Use orchestration tools like Apache Airflow to automate pipelines.\n",
              "- Schedule jobs to run at specific times for regular updates."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from airflow import DAG\n",
              "from airflow.operators.python_operator import PythonOperator\n",
              "from datetime import datetime\n",
              "\n",
              "def my_process():\n",
              "    pass  # data processing code\n",
              "\n",
              "dag = DAG('my_pipeline', description='My data pipeline',\n",
              "          schedule_interval='0 12 * * *',\n",
              "          start_date=datetime(2023, 1, 1), catchup=False)\n",
              "\n",
              "operation = PythonOperator(task_id='process_data', python_callable=my_process, dag=dag)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 12. Data Quality Monitoring\n",
              "- Profile data to monitor quality and detect anomalies.\n",
              "- Set up alerts for quality issues."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from pandas_profiling import ProfileReport\n",
              "import pandas as pd\n",
              "\n",
              "pdf = df.toPandas()\n",
              "profile = ProfileReport(pdf, title='Data Profile')\n",
              "profile.to_file('profile_report.html')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 13. Traceability and Auditing\n",
              "- Track metadata about each pipeline step, including source, transformations, and destination.\n",
              "- Maintain a change history for rollback if needed."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 14. Resource Optimization\n",
              "- Tune Spark resources like memory and cores for performance.\n",
              "- Persist intermediate results to avoid expensive recomputation."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["df.persist()\n",
              "df.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 15. Scalability and Distribution\n",
              "- Partition data intelligently to balance workload across Spark nodes.\n",
              "- Use distributed functions for data-intensive operations."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["df = df.repartition('partition_column')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 16. Security and Compliance\n",
              "- Ensure compliance with regulations such as GDPR or HIPAA.\n",
              "- Use anonymization and masking for sensitive information."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["from pyspark.sql.functions import regexp_replace\n",
              "\n",
              "df = df.withColumn('masked_column', regexp_replace('sensitive_column', '[0-9]', 'X'))"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 17. Collaboration and Code Review\n",
              "- Implement code reviews to maintain quality.\n",
              "- Use Git for versioning and collaboration."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": ["# Basic Git commands\n",
              "# git init\n",
              "# git add .\n",
              "# git commit -m 'Initial commit'\n",
              "# git push origin main"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 18. Documentation and Communication\n",
              "- Keep pipeline documentation up to date, including flow diagrams and usage examples.\n",
              "- Communicate with stakeholders to align pipeline goals with business needs."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 19. A/B Testing and Validation\n",
              "- Use A/B testing to compare pipeline versions and measure impact.\n",
              "- Cross-validation ensures improvements without introducing errors."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 20. Continuous Re-evaluation and Improvement\n",
              "- Periodically review the pipeline for improvements.\n",
              "- Invest in team training to stay current with best practices."]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
