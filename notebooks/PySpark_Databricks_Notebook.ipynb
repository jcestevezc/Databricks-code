{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks PySpark Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains common PySpark commands for Databricks workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"DatabricksPySparkTutorial\").getOrCreate()\n\n# Check Spark version\nprint(spark.version)\n\n# Configure session (example: set driver memory)\nspark = SparkSession.builder.appName(\"DatabricksPySparkTutorial\").config(\"spark.driver.memory\", \"4g\").getOrCreate()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Read a CSV file\ndf = spark.read.csv(\"dbfs:/FileStore/sample_data.csv\", header=True, inferSchema=True)\n\n# Read a Parquet file\ndf_parquet = spark.read.parquet(\"dbfs:/FileStore/sample_data.parquet\")\n\n# Write data to Parquet\ndf.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/output_data\")\n\n# Read from a Databricks table\ndf_table = spark.table(\"default.sample_table\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display first 10 rows\ndf.show(n=10)\n\n# Show schema\ndf.printSchema()\n\n# Select specific columns\ndf.select(\"column1\", \"column2\").show()\n\n# Filter data\ndf_filtered = df.filter(df.column1 > 100)\ndf_filtered.show()\n\n# Group and aggregate\ndf.groupBy(\"column1\").agg({\"column2\": \"sum\"}).show()\n\n# Sort data\ndf.orderBy(\"column1\", ascending=False).show()\n\n# Join DataFrames\ndf_joined = df.join(df_table, df.id == df_table.id, \"inner\")\ndf_joined.show()\n\n# Handle nulls\ndf_no_nulls = df.na.drop()\ndf_filled = df.na.fill(0)\n\n# Create new column\nfrom pyspark.sql.functions import col, lit\ndf = df.withColumn(\"new_column\", col(\"column1\") * 2)\ndf = df.withColumn(\"constant\", lit(100))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SQL in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Register DataFrame as temporary table\ndf.createOrReplaceTempView(\"temp_table\")\n\n# Run SQL query\nresult = spark.sql(\"SELECT column1, COUNT(*) FROM temp_table GROUP BY column1\")\nresult.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common PySpark Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from pyspark.sql.functions import col, when, count, avg, max, min, round, to_date, current_date, datediff\n\n# Conditional column\ndf = df.withColumn(\"category\", when(col(\"column1\") > 100, \"High\").otherwise(\"Low\"))\n\n# Work with dates\ndf = df.withColumn(\"date\", to_date(col(\"date_column\"), \"yyyy-MM-dd\"))\ndf = df.withColumn(\"days_diff\", datediff(current_date(), col(\"date\")))\n\n# User-Defined Function (UDF)\nfrom pyspark.sql.types import StringType\ndef my_function(x):\n    return str(x * 2)\nudf_my_function = udf(my_function, StringType())\ndf = df.withColumn(\"result\", udf_my_function(col(\"column1\")))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimization and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cache DataFrame\ndf.cache()\n\n# Clear cache\ndf.unpersist()\n\n# Repartition data\ndf = df.repartition(10)\ndf = df.coalesce(2)\n\n# View execution plan\ndf.explain()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Databricks-Specific Commands"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# List files in DBFS\ndbutils.fs.ls(\"dbfs:/FileStore/\")\n\n# Create a widget for interactivity\ndbutils.widgets.text(\"parameter\", \"default_value\")\nvalue = dbutils.widgets.get(\"parameter\")\nprint(f\"Widget value: {value}\")\n\n# Example of mounting storage (replace with your credentials)\n# dbutils.fs.mount(\n#     source=\"s3://bucket-name\",\n#     mount_point=\"/mnt/mount_name\",\n#     extra_configs={\"aws_access_key_id\": \"key\", \"aws_secret_access_key\": \"secret\"}\n# )",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\n# Prepare data for ML\nassembler = VectorAssembler(inputCols=[\"column1\", \"column2\"], outputCol=\"features\")\ndf_ml = assembler.transform(df)\n\n# Train linear regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"target\")\nmodel = lr.fit(df_ml)\npredictions = model.transform(df_ml)\npredictions.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Delta Lake Handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Write to Delta table\ndf.write.format(\"delta\").saveAsTable(\"default.delta_table\")\n\n# Optimize Delta table\nspark.sql(\"OPTIMIZE default.delta_table\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Avoid collecting large data\n# df.collect()  # Use only for small DataFrames\ndf.show(5)  # Preferred for large data\n\n# Use efficient formats like Delta\ndf.write.format(\"delta\").save(\"dbfs:/FileStore/delta_table\")",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
