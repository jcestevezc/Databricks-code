{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Silver Layer Data Processing"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["This notebook lists common commands for processing clean and transformed data in the Silver layer."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Initialize Spark"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('silver-processing').getOrCreate()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Data Cleaning"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyspark.sql.functions import col, trim, to_date, regexp_replace, when\n\n# Drop rows with null IDs\nclean_df = raw_df.filter(col('id').isNotNull())\n\n# Trim strings and normalize date and amount formats\nclean_df = (clean_df.withColumn('name', trim(col('name')))\n                     .withColumn('event_date', to_date('event_date', 'MM/dd/yyyy'))\n                     .withColumn('amount', regexp_replace('amount', ',', '').cast('double')))\n\n# Standardize country names\nclean_df = clean_df.withColumn('country', when(col('country') == 'USA', 'United States').otherwise(col('country')))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Advanced Deduplication"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\nwindow_spec = Window.partitionBy('id').orderBy(col('update_time').desc())\nunique_df = (clean_df.withColumn('rn', row_number().over(window_spec))\n                       .filter(col('rn') == 1)\n                       .drop('rn'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Data Enrichment"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Join with reference data\nenriched_df = unique_df.join(dim_country, on='country_code', how='left')\n\n# Add derived columns\nenriched_df = enriched_df.withColumn('age', \n                                     datediff(current_date(), col('birth_date')) / 365)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Structural Transformations"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyspark.sql.functions import from_json, explode, year\nfrom pyspark.sql.types import StructType, StructField, StringType\n\n# Flatten nested JSON column\njson_schema = StructType([StructField('field1', StringType(), True),\n                          StructField('field2', StringType(), True)])\nflat_df = enriched_df.withColumn('json', from_json('json_col', json_schema))\nflat_df = flat_df.select('*', 'json.*').drop('json')\n\n# Pivot example\npivot_df = flat_df.groupBy('id').pivot('status').count()\n\n# Apply strict schema\ncast_df = flat_df.withColumn('year', year('event_date').cast('int'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Filtering"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "filtered_df = cast_df.filter(col('is_active') == True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. Data Quality Validation"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyspark.sql.functions import count, expr\n\n# Validate that amount is positive\ninvalid_amount = filtered_df.filter(col('amount') <= 0).count()\n\n# Record metrics\nrow_count = filtered_df.count()\n\nvalidation_metrics = spark.createDataFrame([(row_count, invalid_amount)], ['row_count', 'invalid_amount'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 8. Optimization for Queries"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "optimized_df = filtered_df.repartition('country').cache()\n\n# Write compressed Delta tables\noptimized_df.write.format('delta').mode('overwrite').option('compression', 'zstd').save('/mnt/silver/clean_table')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
